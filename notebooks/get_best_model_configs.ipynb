{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import hashlib\n",
    "from cxr_fairness.data import Constants\n",
    "import torch\n",
    "from cxr_fairness.metrics import StandardEvaluator\n",
    "import hashlib\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "select_metrics = {\n",
    "  #  'roc': 'max',\n",
    "    'worst_roc': 'max',\n",
    "    # 'roc_gap': 'min'\n",
    "}\n",
    "separating_factors = ['dataset', 'model', 'task'] # select best set for each combination\n",
    "default_group_vars = ['protected_attr', 'subset_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dirs = [Path(\"/scratch/hdd001/home/haoran/cxr_debias/\"),\n",
    "             Path('/scratch/ssd001/home/haoran/cxr_debias/')] # list of directories with trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5609/5609 [20:56<00:00,  4.46it/s]  \n"
     ]
    }
   ],
   "source": [
    "hparams = ['model', 'checkpoint_freq', 'es_patience', 'lr', 'batch_size', 'clf_head_ratio', 'groupdro_eta', 'distmatch_penalty_weight',\n",
    "              'match_type', 'adv_alpha', 'es_metric', 'algorithm', 'data_type', 'fairalm_threshold', 'fairalm_surrogate', 'fairalm_eta',\n",
    "          'JTT_weight', 'JTT_threshold']\n",
    "res = []\n",
    "for i in tqdm([di for project_dir in project_dirs for di in project_dir.glob('**/results.pkl')]):   \n",
    "    args_i = json.load((i.parent/'args.json').open('r'))\n",
    "    args_i['config_filename'] = i.parent.name    \n",
    "    metrics = torch.load(i)['val_metrics']\n",
    "    for metric in select_metrics:\n",
    "        args_i[metric] = metrics[metric]\n",
    "    args_i['hparams_id'] = hashlib.md5(str([args_i[j] for j in hparams if j in args_i]).encode('utf-8')).hexdigest() \n",
    "    res.append(args_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all[~((df_all.data_type == 'balanced') & (df_all.exp_name == 'arl'))]\n",
    "df_all = df_all[~((df_all.data_type == 'balanced') & (df_all.exp_name == 'jtt'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['balanced_concat', 'MMD', 'mean_match', 'single_group',\n",
       "       'erm_baseline_concat', 'dro', 'erm_baseline', 'balanced', 'arl',\n",
       "       'simple_adv', 'jtt', 'fairalm'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_exps = df_all.exp_name.unique()\n",
    "unique_exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1109.000000\n",
       "mean        4.976555\n",
       "std         0.277610\n",
       "min         1.000000\n",
       "25%         5.000000\n",
       "50%         5.000000\n",
       "75%         5.000000\n",
       "max         5.000000\n",
       "Name: val_fold, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "df_all.groupby(['hparams_id', 'exp_name'] + separating_factors + default_group_vars, dropna = False).count()['val_fold'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get best configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['balanced_concat', 'MMD', 'mean_match', 'single_group',\n",
       "       'erm_baseline_concat', 'dro', 'erm_baseline', 'balanced', 'arl',\n",
       "       'simple_adv', 'jtt', 'fairalm'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_exps = df_all.exp_name.unique()\n",
    "unique_exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_configs_raw = [] # best hparams\n",
    "for select_metric in select_metrics:\n",
    "    for exp in unique_exps:\n",
    "        if exp == 'single_group' and select_metric == 'roc_gap':\n",
    "            continue\n",
    "        df = df_all[(df_all.exp_name == exp)]\n",
    "\n",
    "        mean_performance = (\n",
    "            pd.DataFrame(\n",
    "                df\n",
    "                .groupby(['hparams_id'] + separating_factors + default_group_vars, dropna = False)\n",
    "                .agg(performance=(select_metric, 'mean'))\n",
    "                .reset_index()\n",
    "            )\n",
    "        )\n",
    "\n",
    "        best_model = (\n",
    "            mean_performance.groupby(separating_factors + default_group_vars, dropna = False)\n",
    "            .agg(performance=('performance',select_metrics[select_metric])).reset_index()\n",
    "            .merge(mean_performance)\n",
    "            .drop_duplicates(subset = separating_factors + default_group_vars)\n",
    "        )\n",
    "\n",
    "        selected_config = (\n",
    "            best_model[['hparams_id']+ separating_factors + default_group_vars].dropna(axis = 1, how = 'all')\n",
    "            .merge(df)\n",
    "        )\n",
    "        \n",
    "        selected_config['select_metric'] = select_metric\n",
    "        selected_configs_raw.append(selected_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_configs = pd.concat(selected_configs_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_configs_raw = [] # \n",
    "for exp in ['MMD', 'mean_match', 'simple_adv']:\n",
    "    df = df_all[(df_all.exp_name == exp) & (df_all.dataset == 'MIMIC')]\n",
    "    mean_performance = (\n",
    "        pd.DataFrame(\n",
    "            df\n",
    "            .groupby(['hparams_id', 'protected_attr', 'distmatch_penalty_weight', 'adv_alpha', 'task'], dropna = False)\n",
    "            .agg(performance=(select_metric, 'mean'))\n",
    "            .reset_index()\n",
    "        )\n",
    "    )\n",
    "    best_model = (\n",
    "        mean_performance.groupby(['distmatch_penalty_weight', 'adv_alpha', 'protected_attr', 'task'], dropna = False)\n",
    "        .agg(performance=('performance','max')).reset_index()\n",
    "        .merge(mean_performance)\n",
    "        .drop_duplicates(subset = ['protected_attr', 'distmatch_penalty_weight', 'adv_alpha', 'task'])\n",
    "    )\n",
    "    selected_config = (\n",
    "        best_model[['hparams_id', 'task', 'protected_attr']].dropna(axis = 1, how = 'all')\n",
    "        .merge(df)\n",
    "    )\n",
    "    selected_config['select_metric'] = 'vary_lambda_exp'\n",
    "    selected_configs_raw.append(selected_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_exp_name(x):\n",
    "    if x.exp_name == 'simple_adv':\n",
    "        return x.exp_name + '_' + str(x.adv_alpha)\n",
    "    else:\n",
    "        return x.exp_name + '_' + str(x.distmatch_penalty_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.concat(selected_configs_raw)\n",
    "temp.exp_name = temp.apply(add_exp_name, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_configs_final = pd.concat((selected_configs, temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_exps = ['balanced_concat', 'erm_baseline_concat']\n",
    "selected_configs_final = selected_configs_final[~selected_configs_final.exp_name.isin(drop_exps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_configs_final.to_pickle(\n",
    "    project_dirs[0]/'selected_configs.pkl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MMD', 'mean_match', 'single_group', 'dro', 'erm_baseline',\n",
       "       'balanced', 'arl', 'simple_adv', 'jtt', 'fairalm', 'MMD_0.1',\n",
       "       'MMD_0.5', 'MMD_0.75', 'MMD_1.0', 'MMD_2.0', 'MMD_3.0', 'MMD_4.0',\n",
       "       'MMD_5.0', 'MMD_10.0', 'MMD_20.0', 'MMD_25.0', 'MMD_30.0',\n",
       "       'MMD_50.0', 'MMD_100.0', 'mean_match_0.1', 'mean_match_0.5',\n",
       "       'mean_match_0.75', 'mean_match_1.0', 'mean_match_2.0',\n",
       "       'mean_match_3.0', 'mean_match_4.0', 'mean_match_5.0',\n",
       "       'mean_match_10.0', 'mean_match_20.0', 'mean_match_25.0',\n",
       "       'mean_match_30.0', 'mean_match_50.0', 'mean_match_100.0',\n",
       "       'simple_adv_0.01', 'simple_adv_0.05', 'simple_adv_0.1',\n",
       "       'simple_adv_1.0', 'simple_adv_2.0', 'simple_adv_5.0',\n",
       "       'simple_adv_10.0', 'simple_adv_20.0', 'simple_adv_25.0',\n",
       "       'simple_adv_30.0', 'simple_adv_50.0', 'simple_adv_100.0'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_configs_final.exp_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
